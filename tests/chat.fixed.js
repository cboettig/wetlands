// Wetlands Data Chatbot - FIXED VERSION
// Uses MCP SDK for proper SSE communication with MCP server

// Import MCP SDK (these will be loaded via importmap in HTML)
import { Client } from '@modelcontextprotocol/sdk/client/index.js';
import { SSEClientTransport } from '@modelcontextprotocol/sdk/client/sse.js';

class WetlandsChatbot {
    constructor(config) {
        this.config = config;
        this.mcpServerUrl = config.mcp_server_url;
        this.llmEndpoint = config.llm_endpoint;
        this.systemPrompt = null;
        this.messages = [];
        this.mcpClient = null;
        this.mcpTools = [];

        this.initializeUI();
        this.loadSystemPrompt();
        this.initMCP();
    }

    async loadSystemPrompt() {
        try {
            const response = await fetch('system-prompt.md');
            this.systemPrompt = await response.text();
            console.log('‚úì System prompt loaded');
        } catch (error) {
            console.error('Failed to load system prompt:', error);
            this.systemPrompt = 'You are a helpful assistant for wetlands data analysis.';
        }
    }

    async initMCP() {
        try {
            console.log('üîå Initializing MCP connection...');

            // Create SSE transport
            const transport = new SSEClientTransport(new URL(this.mcpServerUrl));

            // Create MCP client
            this.mcpClient = new Client({
                name: 'wetlands-chatbot',
                version: '1.0.0'
            }, {
                capabilities: {}
            });

            // Connect to MCP server
            await this.mcpClient.connect(transport);
            console.log('‚úì MCP client connected');

            // Get available tools
            const toolsResponse = await this.mcpClient.listTools();
            this.mcpTools = toolsResponse.tools;
            console.log('‚úì Available MCP tools:', this.mcpTools.map(t => t.name));

        } catch (error) {
            console.error('‚ùå MCP initialization error:', error);
            this.mcpClient = null;
            this.addMessage('error', 'Failed to connect to database. Some features may not work.');
        }
    }

    initializeUI() {
        // Chat container
        const container = document.createElement('div');
        container.id = 'chat-container';
        container.innerHTML = `
            <div id="chat-header">
                <h3>ü¶Ü Wetlands Data Assistant</h3>
                <button id="chat-toggle">‚àí</button>
            </div>
            <div id="chat-messages"></div>
            <div id="chat-input-container">
                <input type="text" id="chat-input" placeholder="Ask about wetlands data..." />
                <button id="chat-send">Send</button>
            </div>
        `;
        document.body.appendChild(container);

        // Event listeners
        document.getElementById('chat-toggle').addEventListener('click', () => this.toggleChat());
        document.getElementById('chat-send').addEventListener('click', () => this.sendMessage());
        document.getElementById('chat-input').addEventListener('keypress', (e) => {
            if (e.key === 'Enter') this.sendMessage();
        });

        // Welcome message
        this.addMessage('assistant', 'Hi! I can help you explore global wetlands data (GLWDv2.0). Try asking:\n\n ‚Ä¢ "How many hectares of peatlands are there?"\n ‚Ä¢ "What is the total area of freshwater wetlands?"\n ‚Ä¢ "Which wetlands have the highest biodiversity?"\n ‚Ä¢ "Compare coastal vs inland wetland areas"');
    }

    toggleChat() {
        const container = document.getElementById('chat-container');
        const toggle = document.getElementById('chat-toggle');
        container.classList.toggle('collapsed');
        toggle.textContent = container.classList.contains('collapsed') ? '+' : '‚àí';
    }

    addMessage(role, content) {
        const messagesDiv = document.getElementById('chat-messages');
        const messageDiv = document.createElement('div');
        messageDiv.className = `chat-message ${role}`;

        // Use marked.js for markdown rendering
        const formatted = marked.parse(content);
        messageDiv.innerHTML = formatted;
        messagesDiv.appendChild(messageDiv);
        messagesDiv.scrollTop = messagesDiv.scrollHeight;
    }

    async sendMessage() {
        const input = document.getElementById('chat-input');
        const sendButton = document.getElementById('chat-send');
        const userMessage = input.value.trim();

        if (!userMessage) return;

        // Add user message
        this.addMessage('user', userMessage);
        this.messages.push({ role: 'user', content: userMessage });

        // Clear input and disable
        input.value = '';
        input.disabled = true;
        sendButton.disabled = true;

        // Show loading
        this.addMessage('system', 'Analyzing data<span class="loading-dots"></span>');

        try {
            const response = await this.queryLLM(userMessage);

            // Remove loading message
            const messagesDiv = document.getElementById('chat-messages');
            messagesDiv.removeChild(messagesDiv.lastChild);

            // Add assistant response
            this.addMessage('assistant', response);
            this.messages.push({ role: 'assistant', content: response });

        } catch (error) {
            console.error('Chat error:', error);

            // Remove loading message
            const messagesDiv = document.getElementById('chat-messages');
            messagesDiv.removeChild(messagesDiv.lastChild);

            this.addMessage('error', `Error: ${error.message}`);
        } finally {
            input.disabled = false;
            sendButton.disabled = false;
            input.focus();
        }
    }

    async queryLLM(userMessage) {
        if (!this.mcpClient) {
            return "Sorry, the database connection is not available. Please refresh the page to try again.";
        }

        // Build the prompt with system context
        const messages = [
            {
                role: 'system',
                content: this.systemPrompt
            },
            ...this.messages.slice(-10), // Keep last 10 messages for context
            {
                role: 'user',
                content: userMessage
            }
        ];

        // Convert MCP tools to OpenAI function format
        const tools = this.mcpTools.map(tool => ({
            type: 'function',
            function: {
                name: tool.name,
                description: tool.description,
                parameters: tool.inputSchema || {
                    type: 'object',
                    properties: {
                        query: {
                            type: 'string',
                            description: 'SQL query to execute'
                        }
                    },
                    required: ['query']
                }
            }
        }));

        // Call the LLM proxy (API key handled server-side)
        const response = await fetch(this.llmEndpoint, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                model: this.config.llm_model || 'gpt-4',
                messages: messages,
                tools: tools,
                tool_choice: 'auto'
            })
        });

        if (!response.ok) {
            const errorText = await response.text();
            console.error('LLM API Error:', response.status, errorText);
            throw new Error(`LLM API error (${response.status}): ${response.statusText}`);
        }

        const data = await response.json();
        const message = data.choices[0].message;

        // Check if LLM wants to call a tool
        if (message.tool_calls && message.tool_calls.length > 0) {
            const toolCall = message.tool_calls[0];
            const functionArgs = JSON.parse(toolCall.function.arguments);

            // Execute the query via MCP (FIXED - uses SDK now)
            const queryResult = await this.executeMCPQuery(functionArgs.query);

            // Send the result back to LLM for interpretation
            const followUpMessages = [
                ...messages,
                message, // LLM's tool call message
                {
                    role: 'tool',
                    tool_call_id: toolCall.id,
                    content: queryResult
                }
            ];

            const followUpResponse = await fetch(this.llmEndpoint, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    model: this.config.llm_model || 'gpt-4',
                    messages: followUpMessages
                })
            });

            const followUpData = await followUpResponse.json();
            return followUpData.choices[0].message.content;
        }

        return message.content;
    }

    async executeMCPQuery(sqlQuery) {
        if (!this.mcpClient) {
            throw new Error('MCP client not initialized');
        }

        console.log('üîß Executing MCP query:', sqlQuery.substring(0, 100) + '...');

        try {
            // Use MCP SDK to call the tool (FIXED - was manual SSE before)
            const result = await this.mcpClient.callTool({
                name: 'query',
                arguments: {
                    query: sqlQuery
                }
            });

            // Extract text from result
            if (result.content && result.content.length > 0) {
                const text = result.content[0].text;
                console.log('‚úÖ Query result received:', text.substring(0, 200) + '...');
                return text;
            }

            throw new Error('No content in MCP response');

        } catch (error) {
            console.error('‚ùå MCP query error:', error);
            throw new Error(`Database query failed: ${error.message}`);
        }
    }
}

// Initialize chatbot when config is loaded
let chatbot;

// Try loading local config first (for local testing), fall back to production config
fetch('config.local.json')
    .then(response => {
        if (!response.ok) throw new Error('Local config not found');
        return response.json();
    })
    .then(config => {
        console.log('Using local config for testing');
        chatbot = new WetlandsChatbot(config);
        console.log('Wetlands chatbot initialized');
    })
    .catch(() => {
        // Fall back to production config
        fetch('config.json')
            .then(response => response.json())
            .then(config => {
                console.log('Using production config');
                chatbot = new WetlandsChatbot(config);
                console.log('Wetlands chatbot initialized');
            })
            .catch(error => {
                console.error('Failed to load chatbot config:', error);
                // Initialize with default config so UI still appears
                chatbot = new WetlandsChatbot({
                    mcp_server_url: 'https://biodiversity-mcp.nrp-nautilus.io/sse',
                    llm_endpoint: 'https://llm-proxy.nrp-nautilus.io/chat',
                    llm_model: 'qwen3'
                });
            });
    });
