import streamlit as st
from cng.h3 import *
from ibis import _
import importlib
from datetime import time
import openai
import pandas as pd
import traceback

st.set_page_config(layout="wide",
                   page_title="Wetlands",
                   page_icon=":globe:")

from utils import *

'''
# Wetlands
'''
leafmap = importlib.import_module("leafmap.maplibregl")

with st.sidebar:
    m = leafmap.Map(style="positron", use_message_queue=True)
    st.divider()
    style_choice = st.radio("Color by:", style_options)
    paint = style_options[style_choice]
    st.divider()

legend, position, bg_color, fontsize, shape_type, controls = get_legend(paint)
# get all the ids that correspond to the filter

##### Chatbot stuff 
chatbot_container = st.container()
with chatbot_container:
    llm_left_col, llm_right_col = st.columns([5,1], vertical_alignment = "bottom")        
    with llm_right_col:
        llm_choice = st.selectbox("Select LLM:", llm_options, key = "llm", help = "Select which model to use.")   
        llm = llm_options[llm_choice]

from pydantic import BaseModel, Field
class SQLResponse(BaseModel):
    """Defines the structure for SQL response."""
    sql_query: str = Field(description="The SQL query generated by the assistant.")
    explanation: str = Field(description="A detailed explanation of how the SQL query answers the input question.")

with open('app/system_prompt.txt', 'r') as file:
    template = file.read()

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", "{input}")
]).partial(dialect="duckdb", us_wetlands = us_wetlands_z8.schema(), 
           global_wetlands = global_wetlands_z8.schema(),
           carbon = carbon_z8.schema(),
          svi = svi_z8.schema(), mobi = mobi_z8.schema())

structured_llm = llm.with_structured_output(SQLResponse)
few_shot_structured_llm = prompt | structured_llm

@st.cache_data(show_spinner = False)
def run_sql(query, llm_choice):
    """
    Filter data based on an LLM-generated SQL query and return matching IDs.
    Args: query (str): The natural language query to filter the data.
    """
    output = few_shot_structured_llm.invoke(query)
    sql_query = output.sql_query
    explanation =output.explanation
    if not sql_query: # if the chatbot can't generate a SQL query.
        return pd.DataFrame({'ATTRIBUTE' : []}),'', explanation
    result = con.sql(sql_query).distinct().execute()
    if result.empty:
        explanation = "This query did not return any results. Please try again with a different query."
        if 'geom' in result.columns:
            return result.drop('geom',axis = 1), sql_query, explanation 
        else: 
            return result, sql_query, explanation 
    return result, sql_query, explanation
    

with chatbot_container:
    with llm_left_col:
        example_query = "üëã Input query here"
        prompt = st.chat_input(example_query, key="chain", max_chars=300)
    _,log_query_col, _ = st.columns([.001, 5,1], vertical_alignment = "top")
    with log_query_col:
        log_queries = st.checkbox("Save query", value = True, help = "Saving your queries helps improve this tool and guide conservation efforts. Your data is stored in a private location. For more details, see 'Why save your queries?' at the bottom of this page.")
        
# new container for output so it doesn't mess with the alignment of llm options 
with st.container():
    if prompt: 
        st.chat_message("user").write(prompt)
        try:
            with st.chat_message("assistant"):
                with st.spinner("Invoking query..."):
                    llm_output, sql_query, llm_explanation = run_sql(prompt, llm_choice)
                    minio_logger(log_queries, prompt, sql_query, llm_explanation, llm_choice, 'query_log.csv', "shared-tpl")
                    # no sql query generated by chatbot
                    if sql_query == '':
                        st.success(llm_explanation)
                        not_mapping = True
                    else:    
                        # sql query generated but no results returned
                        if llm_output.empty:
                            st.warning(llm_explanation, icon="‚ö†Ô∏è")
                            st.caption("SQL Query:")
                            st.code(sql_query, language="sql")
                            st.stop()
                            
                        # output without mapping columns (id, geom)
                        elif "ATTRIBUTE" not in llm_output.columns and "geom" not in llm_output.columns:
                            st.write(llm_output)
                            not_mapping = True
    
                        # print results 
                        with st.popover("Explanation"):
                            st.write(llm_explanation)
                            if sql_query != '':
                                st.caption("SQL Query:")
                                st.code(sql_query,language = "sql") 
                                
                    # extract ids, columns, bounds if present
                    if "ATTRIBUTE" in llm_output.columns and not llm_output.empty:
                        ids = list(set(llm_output['ATTRIBUTE'].tolist()))
                        llm_cols = extract_columns(sql_query)
                        bounds = llm_output.total_bounds.tolist()
                    else:
                        ids, llm_cols = [], []
                        not_mapping = True

        except Exception as e:
            tb_str = traceback.format_exc()  # full multiline traceback string
            if isinstance(e, openai.BadRequestError):
                st.error(error_messages["bad_request"](llm_choice, e, tb_str), icon="üö®")
                
            elif isinstance(e, openai.RateLimitError):
                st.error(error_messages["bad_request"](llm_choice, e, tb_str), icon="üö®")
            
            elif isinstance(e, openai.APIStatusError):
                st.error(error_messages["bad_request"](llm_choice, e, tb_str), icon="üö®")
            
            elif isinstance(e, openai.InternalServerError):
                st.error(error_messages["internal_server_error"](llm_choice, e, tb_str), icon="üö®")
            
            elif isinstance(e, openai.NotFoundError):
                st.error(error_messages["internal_server_error"](llm_choice, e, tb_str), icon="üö®")
            else:
                prompt = prompt.replace('\n', '')
                st.error(error_messages["unexpected_llm_error"](prompt, e, tb_str))
            st.stop()
##### end of chatbot code 

# define PMTiles style dict (if we didn't already do so using the chatbot)
style=tpl_style_default(paint, pmtiles)

# add pmtiles to map (using user-specified module)
m.add_pmtiles(pmtiles, style=style, 
                  name="National Wetlands Inventory",
                  attribution = "U.S. Fish and Wildlife Service", tooltip=True)
   
m.to_streamlit()
        
